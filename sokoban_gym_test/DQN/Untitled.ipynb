{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sokoban_gym import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from mlagents.envs import UnityEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32,kernel_size=8, stride=4, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,64,kernel_size=4, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64,64,kernel_size=3, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(3136,512)\n",
    "        self.fc_q = nn.Linear(512,5)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0003)\n",
    "    \n",
    "    def Q(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        q = self.fc_q(x)\n",
    "        return q\n",
    "    \n",
    "    def train(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Sokoban')\n",
    "model = DQN()\n",
    "GAMMA = 0.98\n",
    "EPSILON = 0.1\n",
    "BATCH_SIZE = 32\n",
    "N = 30000 ## reply memory size\n",
    "replay_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, eps, action_dim):\n",
    "    if random.random() < eps:\n",
    "        action = random.randint(0, action_dim-1)\n",
    "    else:\n",
    "        action = torch.argmax(Q).item()\n",
    "    return action\n",
    "\n",
    "def store_transition(s, a, r, s_prime, done):\n",
    "    if len(replay_memory) == N:\n",
    "        del(replay_memory[0])\n",
    "    replay_memory.append((s, a, r, s_prime, done))\n",
    "    \n",
    "def training():\n",
    "    s_list, r_list, a_list, s_p_list, done_list = [], [], [], [], []\n",
    "    mini_batch = random.sample(replay_memory, BATCH_SIZE)\n",
    "    for sample in mini_batch:\n",
    "        s_list.append(sample[0].unsqueeze(0))\n",
    "        s_p_list.append(sample[3].unsqueeze(0))\n",
    "        r_list.append([sample[2]])\n",
    "        a_list.append([sample[1]])\n",
    "        done_list.append([0]) if sample[-1] else done_list.append([1])\n",
    "    s = torch.cat(s_list, dim=0)\n",
    "    s_p = torch.cat(s_p_list, dim=0)\n",
    "    a = torch.tensor(a_list).reshape(-1,1)\n",
    "    r = torch.tensor(r_list, dtype=torch.float).reshape(-1,1)\n",
    "    done_mask = torch.tensor(done_list, dtype=torch.float).reshape(-1,1)\n",
    "    \n",
    "    cur_Q = model.Q(s)\n",
    "    next_Q = model.Q(s_p)\n",
    "    td_target = r + GAMMA*torch.max(next_Q, dim=1)[0].reshape(-1,1)*done_mask\n",
    "    loss = (td_target.detach() - cur_Q.gather(1,a)).pow(2).mean()\n",
    "    model.train(loss)\n",
    "\n",
    "def test_agent():\n",
    "    reward_sum = 0.0\n",
    "    for ep in range(10):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            state = torch.tensor(observation, dtype=torch.float)\n",
    "            action = torch.argmax(model.Q(state))\n",
    "            observation, reward, done, _ = env.step(action.item())\n",
    "            reward_sum += reward\n",
    "            if done: break\n",
    "    return reward_sum/10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 , Greedy action reward : -2.010000\n",
      "Episode 39 , Greedy action reward : -3.759000\n",
      "Episode 59 , Greedy action reward : -2.010000\n",
      "Episode 79 , Greedy action reward : -1.909000\n",
      "Episode 99 , Greedy action reward : -1.810000\n",
      "Episode 119 , Greedy action reward : -1.910000\n",
      "Episode 139 , Greedy action reward : -1.792000\n",
      "Episode 159 , Greedy action reward : -1.809000\n",
      "Episode 179 , Greedy action reward : -2.010000\n",
      "Episode 199 , Greedy action reward : -1.800000\n",
      "Episode 219 , Greedy action reward : -1.911000\n",
      "Episode 239 , Greedy action reward : -2.010000\n",
      "Episode 259 , Greedy action reward : -1.709000\n",
      "Episode 279 , Greedy action reward : -1.813000\n",
      "Episode 299 , Greedy action reward : -3.671000\n",
      "Episode 319 , Greedy action reward : -1.811000\n",
      "Episode 339 , Greedy action reward : -1.913000\n",
      "Episode 359 , Greedy action reward : -1.910000\n",
      "Episode 379 , Greedy action reward : -1.888000\n",
      "Episode 399 , Greedy action reward : -3.790000\n",
      "Episode 419 , Greedy action reward : -1.911000\n",
      "Episode 439 , Greedy action reward : -1.911000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-505fe9d3856a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_memory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-afd8d5c3a541>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtd_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_Q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdone_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtd_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcur_Q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-962fe5231522>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ml_agent/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_list = []\n",
    "\n",
    "for ep in range(2000):\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        state = torch.tensor(observation, dtype=torch.float)\n",
    "        action = epsilon_greedy(model.Q(state), EPSILON, 2)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        next_state = torch.tensor(observation, dtype=torch.float)\n",
    "        store_transition(state.squeeze(0), action, reward, next_state.squeeze(0), done)\n",
    "        if len(replay_memory) > 1000:\n",
    "            training()\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    if ep % 20 == 19:\n",
    "        test_reward = test_agent()\n",
    "        print('Episode %d'%ep,', Greedy action reward : %f'%(test_reward))\n",
    "        reward_list.append(test_reward)\n",
    "        if test_reward > 470: break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(10):\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "\n",
    "        state = torch.tensor(observation, dtype=torch.float)\n",
    "        action = torch.argmax(model.Q(state))\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        if done: break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
